{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3bf42e-e64e-4827-b02c-452647d953de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake news sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date label  \n",
       "0  December 31, 2017  fake  \n",
       "1  December 31, 2017  fake  \n",
       "2  December 30, 2017  fake  \n",
       "3  December 29, 2017  fake  \n",
       "4  December 25, 2017  fake  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real news sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date label  \n",
       "0  December 31, 2017   real  \n",
       "1  December 29, 2017   real  \n",
       "2  December 31, 2017   real  \n",
       "3  December 30, 2017   real  \n",
       "4  December 29, 2017   real  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the datasets\n",
    "df_fake = pd.read_csv(\"data/raw/Fake.csv\")\n",
    "df_real = pd.read_csv(\"data/raw/True.csv\")\n",
    "\n",
    "# Add labels\n",
    "df_fake['label'] = 'fake'\n",
    "df_real['label'] = 'real'\n",
    "\n",
    "# Preview\n",
    "print(\"Fake news sample:\")\n",
    "display(df_fake.head())\n",
    "\n",
    "print(\"Real news sample:\")\n",
    "display(df_real.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44b705f6-92d6-4816-86fc-31afc0811872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_7572\\185803023.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['source'] = 'kaggle'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "      <td>fake</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "      <td>real</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "      <td>real</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Le...</td>\n",
       "      <td>fake</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
       "      <td>real</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  source\n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...  fake  kaggle\n",
       "1  Trump drops Steve Bannon from National Securit...  real  kaggle\n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...  real  kaggle\n",
       "3   OOPS: Trump Just Accidentally Confirmed He Le...  fake  kaggle\n",
       "4  Donald Trump heads for Scotland to reopen a go...  real  kaggle"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine datasets\n",
    "df = pd.concat([df_fake, df_real], ignore_index=True)\n",
    "\n",
    "# Combine title and text into one column\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['full_text'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "# Keep only relevant columns\n",
    "df_final = df[['full_text', 'label']]\n",
    "df_final.columns = ['text', 'label']\n",
    "\n",
    "# Add source column\n",
    "df_final['source'] = 'kaggle'\n",
    "\n",
    "# Shuffle the data\n",
    "data_1 = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Preview\n",
    "data_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5c9b8e4-2d78-417a-9f2c-2c98ffd78261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['politicsNews' 'worldnews']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['News', 'politics', 'Government News', 'left-news', 'US_News',\n",
       "       'Middle-east'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_real['subject'].unique())\n",
    "df_fake['subject'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dcda6cf-a3b2-488c-8cc0-2556fe325e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_7940\\1223288417.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['source'] = 'kaggle'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Le...</td>\n",
       "      <td>News</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       subject  source\n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...       US_News  kaggle\n",
       "1  Trump drops Steve Bannon from National Securit...  politicsNews  kaggle\n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...  politicsNews  kaggle\n",
       "3   OOPS: Trump Just Accidentally Confirmed He Le...          News  kaggle\n",
       "4  Donald Trump heads for Scotland to reopen a go...  politicsNews  kaggle"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine datasets\n",
    "df = pd.concat([df_fake, df_real], ignore_index=True)\n",
    "\n",
    "# Combine title and text into one column\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['full_text'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "# Keep only relevant columns\n",
    "df_final = df[['full_text', 'subject']]\n",
    "df_final.columns = ['text', 'subject']\n",
    "\n",
    "# Add source column\n",
    "df_final['source'] = 'kaggle'\n",
    "\n",
    "# Shuffle the data\n",
    "data_2 = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Preview\n",
    "data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f9a3c10-dc09-4263-a44c-b7b8f88c7141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Kaggle dataset saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save to processed folder\n",
    "data_1.to_csv(\"data/processed/kaggle_clean.csv\", index=False)\n",
    "print(\"Cleaned Kaggle dataset saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5321993-8c65-4f13-ab88-f07494ea20cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Kaggle dataset saved successfully.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44898 entries, 0 to 44897\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     44898 non-null  object\n",
      " 1   subject  44898 non-null  object\n",
      " 2   source   44898 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Save to processed folder\n",
    "data_2.to_csv(\"data/processed/kaggle_clean.csv_2\", index=False)\n",
    "print(\"Cleaned Kaggle dataset saved successfully.\")\n",
    "data_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbb3f33e-8921-4e9c-972f-11b31ef1ed84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44898 entries, 0 to 44897\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    44898 non-null  object\n",
      " 1   label   44898 non-null  object\n",
      " 2   source  44898 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c25d952b-9274-4f08-9508-7f1ae03c7785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (4.13.4)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (11.2.1)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (1.3.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (5.4.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (2.32.4)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (5.3.0)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.14.0)\n",
      "Requirement already satisfied: six in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2025.4.26)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d322fcb7-2e88-40d3-980a-bdb78bfcb833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml_html_clean in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lxml_html_clean) (5.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml_html_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c80b2eb-e762-4cc2-b422-286a14ea0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e64d64-59d6-4aeb-95f0-e4c72ed65e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 1 done. 15001 rows survived → gossipcop_urls_step1_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load first 200 rows for testing (adjust for full later)\n",
    "df = pd.read_csv(\"data/raw/gossipcop_real.csv\")\n",
    "\n",
    "# --- Clean malformed or blank URLs ---\n",
    "def clean_url(url):\n",
    "    if pd.isna(url) or not isinstance(url, str) or len(url.strip()) < 10:\n",
    "        return None\n",
    "    url = url.strip()\n",
    "    if not url.startswith(\"http\"):\n",
    "        url = \"https://\" + url.lstrip(\":/\")\n",
    "    return url\n",
    "\n",
    "df['clean_url'] = df['news_url'].apply(clean_url)\n",
    "df = df.dropna(subset=['clean_url'])\n",
    "\n",
    "# --- Filter Out Non-Responsive or Low-Value Domains ---\n",
    "blocklist = [\n",
    "    'wikipedia.org', 'quora.com', 'msn.com', 'kisscasper.com', 'dadli.mobi',\n",
    "    'longroom.com', 'celebrityinsider.org', 'gossipbucket.com', 'newmediasearch.com',\n",
    "    'statista.com', 'refinery29.com/en-us/2017', 'trueara.com', 'pennews.pencidesign.com',\n",
    "    'medium.com/@AndreAguirre25111NTZ', 'article.wn.com/view/2017', 'storiesflow.com',\n",
    "    'thisisinsider.com', 'nickiswift.com', 'longroom.com', 'cmch.tv', 'statista.com',\n",
    "    'acriticalreviewofthehelp.wordpress.com'\n",
    "]\n",
    "\n",
    "# Remove URLs containing any blocked domain\n",
    "pattern = '|'.join(blocklist)\n",
    "df = df[~df['clean_url'].str.contains(pattern, na=False)]\n",
    "\n",
    "# Save for next stage\n",
    "df.to_csv(\"data/processed/gossipcop_urls_step1_cleaned.csv\", index=False)\n",
    "print(f\"✅ Step 1 done. {len(df)} rows survived → gossipcop_urls_step1_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5941af20-964b-4818-971d-7ddae9d09588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text: 100%|███████████████████████████████████████████████████████████| 5000/5000 [1:47:59<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 2 done. 1567 rows saved to gossipcop_final_cleaned_real.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load filtered URLs\n",
    "df = pd.read_csv(\"data/processed/gossipcop_urls_step1_cleaned.csv\").head(5000)\n",
    "df = df.copy()\n",
    "\n",
    "# Extract full article text\n",
    "def extract_text(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text.strip()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "tqdm.pandas(desc=\"Extracting text\")\n",
    "df['text'] = df['clean_url'].progress_apply(extract_text)\n",
    "\n",
    "# Filter out rows with very short or empty text\n",
    "df = df[df['text'].str.len() >= 100]\n",
    "\n",
    "# Save final file\n",
    "df.to_csv(\"data/processed/gossipcop_final_cleaned_real.csv\", index=False)\n",
    "print(f\"✅ Step 2 done. {len(df)} rows saved to gossipcop_final_cleaned_real.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258b74e6-d02d-4216-97aa-4bb8763af070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text:  45%|█████████████████████████▉                               | 2272/5000 [1:40:50<1:17:41,  1.71s/it]C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname PST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "Extracting text: 100%|███████████████████████████████████████████████████████████| 5000/5000 [4:07:04<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 2 done. 3302 rows saved to gossipcop_final_cleaned_real_2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load filtered URLs\n",
    "df = pd.read_csv(\"data/processed/gossipcop_urls_step1_cleaned.csv\").iloc[5000:10000].copy()\n",
    "df = df.copy()\n",
    "\n",
    "# Extract full article text\n",
    "def extract_text(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text.strip()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "tqdm.pandas(desc=\"Extracting text\")\n",
    "df['text'] = df['clean_url'].progress_apply(extract_text)\n",
    "\n",
    "# Filter out rows with very short or empty text\n",
    "df = df[df['text'].str.len() >= 100]\n",
    "\n",
    "# Save final file\n",
    "df.to_csv(\"data/processed/gossipcop_final_cleaned_real_1.csv\", index=False)\n",
    "print(f\"✅ Step 2 done. {len(df)} rows saved to gossipcop_final_cleaned_real_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34877db3-b252-4e0c-afe6-1ea09874c8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text:  46%|██████████████████████████▍                              | 2324/5001 [2:13:54<2:24:49,  3.25s/it]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Extracting text: 100%|███████████████████████████████████████████████████████████| 5001/5001 [4:24:13<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 2 done. 3214 rows saved to gossipcop_final_cleaned_real_2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load filtered URLs\n",
    "df = pd.read_csv(\"data/processed/gossipcop_urls_step1_cleaned.csv\").iloc[10000:15001].copy()\n",
    "df = df.copy()\n",
    "\n",
    "# Extract full article text\n",
    "def extract_text(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text.strip()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "tqdm.pandas(desc=\"Extracting text\")\n",
    "df['text'] = df['clean_url'].progress_apply(extract_text)\n",
    "\n",
    "# Filter out rows with very short or empty text\n",
    "df = df[df['text'].str.len() >= 100]\n",
    "\n",
    "# Save final file\n",
    "df.to_csv(\"data/processed/gossipcop_final_cleaned_real_2.csv\", index=False)\n",
    "print(f\"✅ Step 2 done. {len(df)} rows saved to gossipcop_final_cleaned_real_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8cc5cf-73d2-4a2c-9668-8c0d44c2be0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load full dataset\n",
    "df = pd.read_csv(\"data/raw/gossipcop_fake.csv\")\n",
    "\n",
    "# Step 1: Clean and standardize URLs\n",
    "def clean_url(url):\n",
    "    if pd.isna(url) or not isinstance(url, str):\n",
    "        return None\n",
    "    url = url.strip()\n",
    "    if not url.startswith(\"http\"):\n",
    "        url = \"https://\" + url.lstrip(\":/\")\n",
    "    return url\n",
    "\n",
    "df['clean_url'] = df['news_url'].apply(clean_url)\n",
    "blacklist = ['yournewswire.com', 'en.wikipedia.org', 'msn.com']\n",
    "df = df.dropna(subset=['clean_url'])\n",
    "df = df[~df['clean_url'].str.contains('|'.join(blacklist), na=False)]\n",
    "\n",
    "# Step 2: Extract meaningful text\n",
    "def extract_text(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = article.text.strip()\n",
    "        if len(text) < 200 or text.lower().startswith((\"updated\", \"click here\")):\n",
    "            return None\n",
    "        return text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['text'] = df['clean_url'].apply(lambda url: extract_text(url))\n",
    "df = df.dropna(subset=['text'])\n",
    "df = df.drop_duplicates(subset='title')\n",
    "\n",
    "# Step 3: Save final cleaned dataset\n",
    "df.to_csv(\"data/processed/fakenewsnet_gossipcop_cleaned.csv\", index=False)\n",
    "print(f\"✅ Done. Saved: {len(df)} rows → fakenewsnet_gossipcop_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381a6f9e-8559-4b9b-a7a2-f77211fa0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined and saved. Total rows: 10741\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_1 = pd.read_csv(\"data/processed/gossipcop_final_cleaned_real.csv\", encoding='latin1')\n",
    "df_2 = pd.read_csv(\"data/processed/gossipcop_final_cleaned_real_1.csv\", encoding='latin1')\n",
    "df_3 = pd.read_csv(\"data/processed/gossipcop_final_cleaned_real_2.csv\", encoding='latin1')\n",
    "df_0 = pd.read_csv(\"data/processed/fakenewsnet_gossipcop_cleaned.csv\", encoding='latin1')\n",
    "\n",
    "\n",
    "# Add labels\n",
    "df_1[\"label\"] = \"real\"\n",
    "df_2[\"label\"] = \"real\"\n",
    "df_3[\"label\"] = \"real\"\n",
    "df_0[\"label\"] = \"fake\"\n",
    "\n",
    "# Combine all\n",
    "df_final = pd.concat([df_1, df_2, df_3, df_0], ignore_index=True)\n",
    "\n",
    "# Shuffle the rows\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the final file\n",
    "df_final.to_csv(\"data/processed/gossipcop_final.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Combined and saved. Total rows: {len(df_final)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde5affe-34ad-44b7-b2f5-4775fe53b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10741 entries, 0 to 10740\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         10741 non-null  object\n",
      " 1   news_url   10741 non-null  object\n",
      " 2   title      10740 non-null  object\n",
      " 3   tweet_ids  10206 non-null  object\n",
      " 4   clean_url  10738 non-null  object\n",
      " 5   text       10738 non-null  object\n",
      " 6   label      10741 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 587.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a1e2201-8a61-47fa-a657-008762dd1dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned and saved. Final row count: 10738\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10738 entries, 0 to 10740\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    10738 non-null  object\n",
      " 1   text     10738 non-null  object\n",
      " 2   label    10738 non-null  object\n",
      " 3   subject  10738 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 419.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_csv(\"data/processed/gossipcop_final.csv\", encoding=\"latin1\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=[\"id\", \"news_url\", \"tweet_ids\", \"clean_url\"])\n",
    "\n",
    "# Drop rows where 'text' is null\n",
    "df = df.dropna(subset=[\"text\"])\n",
    "\n",
    "# Add subject column\n",
    "df[\"subject\"] = \"entertainment\"\n",
    "\n",
    "# Save the cleaned file\n",
    "df.to_csv(\"data/processed/gossipcop_final_labeled.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Cleaned and saved. Final row count: {len(df)}\")\n",
    "\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05c4294-e2e2-4027-a26a-c35dd862e2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned and saved. Final row count: 9879\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9879 entries, 0 to 9878\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    9879 non-null   object\n",
      " 1   text     9879 non-null   object\n",
      " 2   label    9879 non-null   object\n",
      " 3   subject  9879 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 308.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data_1 = pd.read_csv(\"data/processed/clean_useabel_file_ gossipcop_ lebled.csv\")\n",
    "\n",
    "# Full cleanup\n",
    "data_1 = data_1.dropna()\n",
    "data_1 = data_1[data_1['text'].str.len() >= 100]\n",
    "data_1 = data_1.drop_duplicates(subset='text')\n",
    "\n",
    "# Fix index\n",
    "data_1 = data_1.reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ Cleaned and saved. Final row count: {len(data_1)}\")\n",
    "data_1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81af724a-4fa3-4f74-82a5-2086c93c80ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9879 entries, 0 to 9878\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    9879 non-null   object\n",
      " 1   label   9879 non-null   object\n",
      " 2   source  9879 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 231.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 'data/processed/gossipcop_formatted_for_model.csv')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine title and text into one column\n",
    "data_1[\"text\"] = data_1[\"title\"].astype(str) + \". \" + data_1[\"text\"].astype(str)\n",
    "\n",
    "# Drop the original title and subject columns\n",
    "data_1.drop(columns=[\"title\", \"subject\"], inplace=True)\n",
    "\n",
    "# Add a source column\n",
    "data_1[\"source\"] = \"gossipcop\"\n",
    "\n",
    "# Save the cleaned and formatted version\n",
    "output_path = \"data/processed/gossipcop_formatted_for_model.csv\"\n",
    "data_1.to_csv(output_path, index=False)\n",
    "\n",
    "# Display final structure and count\n",
    "data_1.info(), output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea5002c-a56f-43d6-a31f-c78c038826b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9879 entries, 0 to 9878\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     9879 non-null   object\n",
      " 1   subject  9879 non-null   object\n",
      " 2   source   9879 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 231.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 'data/processed/gossipcop_formatted_for_model.csv_2')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine title and text into one column\n",
    "data_1[\"text\"] = data_1[\"title\"].astype(str) + \". \" + data_1[\"text\"].astype(str)\n",
    "\n",
    "# Drop the original title and subject columns\n",
    "data_1.drop(columns=[\"title\", \"label\"], inplace=True)\n",
    "\n",
    "# Add a source column\n",
    "data_1[\"source\"] = \"gossipcop\"\n",
    "\n",
    "# Save the cleaned and formatted version\n",
    "output_path = \"data/processed/gossipcop_formatted_for_model.csv_2\"\n",
    "data_1.to_csv(output_path, index=False)\n",
    "\n",
    "# Display final structure and count\n",
    "data_1.info(), output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7853df6-8e0f-4c1e-8a82-76de9b23af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined dataset saved with 54777 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "data_1 = pd.read_csv(\"data/processed/gossipcop_formatted_for_model.csv\")\n",
    "data_2 = pd.read_csv(\"data/processed/kaggle_clean.csv\")\n",
    "\n",
    "# Combine\n",
    "combined = pd.concat([data_1, data_2], ignore_index=True)\n",
    "\n",
    "# Optional: shuffle\n",
    "combined = combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "combined.to_csv(\"data/processed/combined_fake_news_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Combined dataset saved with {len(combined)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79a75b7e-590d-4858-842a-d7e5704b6b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54777 entries, 0 to 54776\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    54777 non-null  object\n",
      " 1   label   54777 non-null  object\n",
      " 2   source  54777 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffe034a-e9be-4be7-bab6-ec9e13a0a344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined dataset saved with 54777 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "data_1 = pd.read_csv(\"data/processed/gossipcop_formatted_for_model.csv_2\")\n",
    "data_2 = pd.read_csv(\"data/processed/kaggle_clean.csv_2\")\n",
    "\n",
    "# Combine\n",
    "combined = pd.concat([data_1, data_2], ignore_index=True)\n",
    "\n",
    "# Optional: shuffle\n",
    "combined = combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "combined.to_csv(\"data/processed/combined_news_type_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Combined dataset saved with {len(combined)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f860a48-3430-487c-83c2-46b9d17e78df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54777 entries, 0 to 54776\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     54777 non-null  object\n",
      " 1   subject  54777 non-null  object\n",
      " 2   source   54777 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31fd0ff-0540-4f2b-83bf-b817080e60da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
